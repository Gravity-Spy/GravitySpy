#!/usr/bin/env python

import pandas as pd
import numpy as np
import os, sys
import pickle
import pdb
import argparse
import pandas

from sqlalchemy.engine import create_engine
from gravityspy.api import params
from gravityspy.api.project import GravitySpyProject
from tqdm import tqdm
from multiprocessing import Pool

engine = create_engine('postgresql://{0}:{1}@gravityspyplus.ciera.northwestern.edu:5432/gravityspy'.format(os.environ['GRAVITYSPYPLUS_DATABASE_USER'],os.environ['GRAVITYSPYPLUS_DATABASE_PASSWD']))
priors = params.Priors()
weighting = params.Weighting()

### Argument handling ###

argp = argparse.ArgumentParser()
argp.add_argument("-f", "--file-name", default='', type=str, help="File stem for output data")
argp.add_argument("--output", required=True, type=str, help="Where do you want the results to go?")
argp.add_argument("-p", "--project-pickle", type=str, help="Project information pickle")
argp.add_argument("-nc", "--num-cores", default=1, type=int, help="Specify the number of cores that the retirement code will be parallelized over")
argp.add_argument("--min-label", default=1, type=int, help="Minimum number of citizen labels that an image must receive before it is retired. Default=1")
argp.add_argument("--max-label", default=50, type=int, help="Maximum number of citizen labels that an image must receive before it is retired as NOA. Default=50")
argp.add_argument("--ret-thresh", default=0.8, help="Retirement threshold that must be achieved to retire a particular class. Can be a float, or a 22-length vector of floats. Default = 0.8")
argp.add_argument("--prior", default='uniform', type=str, help="String indicating the prior choice for the subjects. Calls function from class params.py. Default=uniform")
argp.add_argument("--weighting", default='default', type=str, help="String indicating the weighting choice for the subjects. Calls function from class params.py. Default=default, where all users receive an equal weight")
args = argp.parse_args()

# Obtain number of classes from API
gspyproject = GravitySpyProject.load_project_from_cache(args.project_pickle)
workflowDictSubjectSets = gspyproject.get_level_structure(IDfilter='O2')
classes = sorted(workflowDictSubjectSets["7766"].keys())
print(classes)

# From ansers Dict determine number of classes
numClasses = len(classes)
print(numClasses)

# Flat retirement criteria #FIXME make this work for vector of thresholds
ret_thresh = float(args.ret_thresh)*np.ones(numClasses)

# Flat priors b/c we do not know what category the image is in #FIXME make this work for other defined priors
if args.prior == 'uniform':
    prior = priors.uniform(numClasses)

# Load classifications
print('\nreading classifications...')
query = ("SELECT classificationsdev.id, classificationsdev.links_user, "
        "classificationsdev.links_subjects, classificationsdev.links_workflow, "
        "classificationsdev.\"annotations_value_choiceINT\", retirement_project_glitches.gravityspy_id, "
        "retirement_project_glitches.ml_confidence, retirement_project_glitches.ml_label, "
        "retirement_project_glitches.\"1400Ripples\", retirement_project_glitches.\"1080Lines\", retirement_project_glitches.\"Air_Compressor\", retirement_project_glitches.\"Blip\", retirement_project_glitches.\"Chirp\", retirement_project_glitches.\"Extremely_Loud\", retirement_project_glitches.\"Helix\", retirement_project_glitches.\"Koi_Fish\", "
        "retirement_project_glitches.\"Light_Modulation\", retirement_project_glitches.\"Low_Frequency_Burst\", retirement_project_glitches.\"Low_Frequency_Lines\", retirement_project_glitches.\"No_Glitch\" ,retirement_project_glitches.\"None_of_the_Above\", retirement_project_glitches.\"Paired_Doves\", retirement_project_glitches.\"Power_Line\", retirement_project_glitches.\"Repeating_Blips\", retirement_project_glitches.\"Scattered_Light\", retirement_project_glitches.\"Scratchy\", retirement_project_glitches.\"Tomte\", retirement_project_glitches.\"Violin_Mode\", retirement_project_glitches.\"Wandering_Line\", retirement_project_glitches.\"Whistle\" "
        "FROM classificationsdev INNER JOIN retirement_project_glitches ON classificationsdev.links_subjects = retirement_project_glitches.links_subjects "
        "WHERE classificationsdev.\"annotations_value_choiceINT\" != -1 AND "
        "classificationsdev.\"annotations_value_choiceINT\" != 12 AND "
        "classificationsdev.links_user != 0")

classifications = pandas.read_sql(query, engine)
classifications = classifications.sort_values('id')
# If a user has classifiied a glitch more than once, use earliest classification
classifications.drop_duplicates(['links_subjects','links_user'], keep='first', inplace=True)

# Load confusion matrices
print("Calculating the confusion matrix timeseries for each user")
huge_dict = gspyproject.calculate_confusion_matrices_per_classification()

# Create imageDB
columnsForImageDB = sorted(workflowDictSubjectSets["7766"].keys())
columnsForImageDB.extend(['gravityspy_id','links_subjects','ml_confidence','ml_label','id'])
image_db = classifications[columnsForImageDB].drop_duplicates(['links_subjects'])
image_db.set_index(['links_subjects'],inplace=True)
image_db['numLabel'] = 0
image_db['retired'] = 0
image_db['numClassifications'] = 0
image_db['finalScore'] = 0.0
image_db['finalLabel'] = ''
image_db['cumWeight'] = 0.0

def extract_user_conf_matrix(user_id, classification_id):
    user_conf_matrix_time_series = huge_dict[user_id]
    classification_list = list(user_conf_matrix_time_series.keys())
    conf_index = np.searchsorted(classification_list, classification_id)
    conf_index = max(0, conf_index-1)
    conf_matrix = user_conf_matrix_time_series[classification_list[conf_index]]
    return conf_matrix.toarray()

def get_post_contribution(x):
    # NOTE: the variable 'x' is the subject link
    # find all classifications for a particular subject
    glitch = classifications[classifications.links_subjects==x]
    # make sure that all users who classified this glitch have a confusion matrix.
    glitch = glitch.loc[glitch.links_user.isin(huge_dict.keys())]
    # obtain confusion amtrices
    glitch["conf_matrices_for_this_glitch"] = glitch.apply(lambda x: extract_user_conf_matrix(x["links_user"], x["id"]), axis=1)
    # sort based on when the classification was made
    glitch = glitch.sort_values('id')
    # counter to keep track of the weighting normalization, starts at 1.0 for machine
    weight_ctr = 1.0
    # track the contribution of each user towards retirement, initialize with ML score
    tracker = np.atleast_2d(glitch.iloc[0][classes].values)


    # Loop through all people that classified, or until retirement criteria are met
    for idx, person in enumerate(glitch.links_user):

        # Check for maximum number of labels
        if image_db.loc[x, 'numLabel'] > args.max_label:
            image_db.loc[x, 'numClassifications'] = args.max_label
            image_db.loc[x, 'finalScore'] = posterior.divide(weight_ctr).max()
            image_db.loc[x, 'finalLabel'] = classes[np.asarray(posterior.divide(weight_ctr)).argmax()]
            image_db.loc[x, 'tracks'] = [tracker]
            return

        # grab first person's annotation of the glitch
        classification = glitch[glitch.links_user == person]
        matrix = classification.conf_matrices_for_this_glitch.iloc[0]
        # find the row associated with the annotation the user made
        row = int(classification.annotations_value_choiceINT)
        # get the unweighted posterior contribution
        post_contribution = matrix/np.sum(matrix, axis=1)

        # If this user hasn't classified any golden images of this type, move on
        if np.isnan(post_contribution[row,:]).any():
            # if this is the last user in the group, save pertinent info
            if idx == len(glitch)-1:
                # first, make sure someone has contributed to the posterior for this image
                try:
                    posterior
                except NameError:
                    continue
                image_db.loc[x, 'numClassifications'] = image_db.loc[x, 'numLabel']
                image_db.loc[x, 'finalScore'] = posterior.divide(weight_ctr).max()
                image_db.loc[x, 'finalLabel'] = classes[np.asarray(posterior.divide(weight_ctr)).argmax()]
                image_db.loc[x, 'tracks'] = [tracker]
                return
            else:
                continue

        # calculate the weight of the user for this glitch classification
        weight = weighting.default(classifications, person, x)
        # keep track of weighting counter for normalization purposes
        weight_ctr += weight
        # grab the posterior contribution for that class, weighted by classification weight
        posteriorToAdd = weight*post_contribution[row, :]
        # concatenate the new posterior contribution to tracker
        tracker = np.vstack((tracker, posteriorToAdd))
        # update image_db with the posterior contribution
        image_db.loc[x, classes] = image_db.loc[x, classes].add(np.asarray(posteriorToAdd).squeeze())
        # add 1 to numLabels for all images
        image_db.loc[x, 'numLabel'] = image_db.loc[x, 'numLabel'] + 1
        # get the total (unnormalized) posterior contribution at this point
        posterior = image_db.loc[x][classes]

        # Check for retirement threshold and minimum number of labels
        if ((posterior.divide(weight_ctr) > ret_thresh).any() and image_db.loc[x, 'numLabel'] >= args.min_label):
            # save pertinent info of the retired image
            image_db.loc[x, classes] = image_db.loc[x, classes].divide(weight_ctr)
            image_db.loc[x, 'numClassifications'] = image_db.loc[x, 'numLabel']
            image_db.loc[x, 'finalScore'] = posterior.divide(weight_ctr).max()
            image_db.loc[x, 'finalLabel'] = classes[np.asarray(posterior.divide(weight_ctr)).argmax()]
            image_db.loc[x, 'retired'] = 1
            image_db.loc[x, 'cumWeight'] = weight_ctr
            image_db.loc[x, 'tracks'] = [tracker]
            return

       # If all people have been accounted for and image not retired, save info to image_db and tracks
        if idx == len(glitch.links_user)-1:
            image_db.loc[x, 'numClassifications'] = image_db.loc[x, 'numLabel']
            image_db.loc[x, 'finalScore'] = posterior.divide(weight_ctr).max()
            image_db.loc[x, 'finalLabel'] = classes[np.asarray(posterior.divide(weight_ctr)).argmax()]
            image_db.loc[x, 'tracks'] = [tracker]
            return


print('determining retired images...')
# sort data based on subjects number
subjects = classifications.links_subjects.unique()
subjects.sort()

if not os.path.isdir(args.output):
    os.makedirs(args.output)

# implementation for multiprocessing
with Pool(args.num_cores) as pool:
    #image_db = image_db.loc[subjects]

    pool.map(get_post_contribution, subjects)

    # save image and retirement data as hdf5 files
    image_db.to_hdf('{0}/retired_{1}.hdf5'.format(args.output, args.file_name), key='retired')

